{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- EvaCun Translation Environment bootstrap (run first) ---\n",
    "# Clones and uses your EvaCun-Colab-Notebook repo so paths/imports resolve in Colab or local Jupyter.\n",
    "import os, sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect Colab\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Defaults to your repo; override with EVACUN_APP_REPO_URL if needed\n",
    "DEFAULT_URL = \"https://github.com/ancient-world-citation-analysis/EvaCun-Colab-Notebook.git\"\n",
    "REPO_URL = os.environ.get(\"EVACUN_APP_REPO_URL\", DEFAULT_URL).strip()\n",
    "REPO_NAME = os.environ.get(\"EVACUN_APP_REPO_NAME\", \"EvaCun-Colab-Notebook\").strip()\n",
    "REPO_DIR = Path(os.getenv(\"EVACUN_APP_REPO_DIR\", Path.cwd() / REPO_NAME)).resolve()\n",
    "\n",
    "# Data layout\n",
    "DATA_DIR = REPO_DIR / \"data\"\n",
    "INPUT_DIR = DATA_DIR / \"input\"\n",
    "OUTPUT_DIR = DATA_DIR / \"outputs\"\n",
    "INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Clone if missing/empty\n",
    "if not REPO_DIR.exists() or not any(REPO_DIR.iterdir()):\n",
    "    print(f\"Cloning {REPO_URL} into {REPO_DIR} ...\")\n",
    "    subprocess.check_call([\"git\", \"clone\", \"--depth\", \"1\", REPO_URL, str(REPO_DIR)])\n",
    "\n",
    "# Install if it looks like a Python project\n",
    "pyproject = REPO_DIR / \"pyproject.toml\"\n",
    "setup_py = REPO_DIR / \"setup.py\"\n",
    "if pyproject.exists() or setup_py.exists():\n",
    "    print(\"Installing EvaCun app repo (editable if possible)...\")\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", str(REPO_DIR)])\n",
    "    except subprocess.CalledProcessError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", str(REPO_DIR)])\n",
    "\n",
    "# Add repo to path and cd there so relative imports and paths work\n",
    "if str(REPO_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_DIR))\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "print(\"Working dir:\", REPO_DIR)\n",
    "print(\"INPUT_DIR:\", INPUT_DIR)\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n",
    "\n",
    "# Install extra deps if present\n",
    "for req_name in [\"requirements-colab.txt\", \"requirements.txt\"]:\n",
    "    req_path = REPO_DIR / req_name\n",
    "    if req_path.exists():\n",
    "        print(f\"Installing dependencies from {req_name} ...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_path)])\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Warning: failed to install from {req_name}: {e}\")\n",
    "\n",
    "# Convenience helpers for I/O\n",
    "from pathlib import Path as _Path\n",
    "import pandas as _pd\n",
    "\n",
    "def in_input(*parts): return INPUT_DIR.joinpath(*parts)\n",
    "def in_output(*parts): return OUTPUT_DIR.joinpath(*parts)\n",
    "\n",
    "def read_csv(path, **kwargs):\n",
    "    path = _Path(path); print(\"Reading CSV:\", path); return _pd.read_csv(path, **kwargs)\n",
    "\n",
    "def write_csv(df, path, **kwargs):\n",
    "    path = _Path(path); path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Writing CSV:\", path); return df.to_csv(path, index=False, **kwargs)\n",
    "\n",
    "print(\"EvaCun translation bootstrap complete.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
  "cell_type": "code",
  "metadata": {},
  "source": [
    "# --- Fetch EvaCun datasets from Zenodo (pin to a VERSION DOI for reproducibility) ---\n",
    "import json, hashlib\n",
    "from pathlib import Path\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "\n",
    "ZENODO_DOI_VERSION = \"10.5281/zenodo.XXXXXXX\"  # <-- replace with your VERSION DOI after publishing\n",
    "DATA_DIR = (Path.cwd() / \"data\" / \"input\").resolve()\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Map desired filenames; we'll fill URLs from the Zenodo record\n",
    "NEEDED = {\n",
    "    \"akkadian_train.txt\": None,\n",
    "    \"transcription_train.txt\": None,\n",
    "    \"english_train.txt\": None,\n",
    "    \"akkadian_validation.txt\": None,\n",
    "    \"transcription_validation.txt\": None,\n",
    "    \"english_validation.txt\": None,\n",
    "    # Optional: checksums file if uploaded\n",
    "    \"SHA256SUMS.txt\": None,\n",
    "}\n",
    "\n",
    "def zenodo_record_from_doi(doi: str) -> dict:\n",
    "    # Resolve DOI to Zenodo record URL, then fetch JSON\n",
    "    with urlopen(f\"https://doi.org/{doi}\") as r:\n",
    "        rec_url = r.geturl()\n",
    "    rec_id = rec_url.rstrip(\"/\").split(\"/\")[-1]\n",
    "    with urlopen(f\"https://zenodo.org/api/records/{rec_id}\") as r:\n",
    "        return json.load(r)\n",
    "\n",
    "rec = zenodo_record_from_doi(ZENODO_DOI_VERSION)\n",
    "\n",
    "# Discover direct file URLs\n",
    "for f in rec.get(\"files\", []):\n",
    "    name = f.get(\"key\")\n",
    "    if name in NEEDED:\n",
    "        NEEDED[name] = f[\"links\"][\"self\"]\n",
    "\n",
    "# Download files if missing\n",
    "for fname, url in NEEDED.items():\n",
    "    if url is None:\n",
    "        continue\n",
    "    dest = DATA_DIR / fname\n",
    "    if dest.exists() and dest.stat().st_size > 0:\n",
    "        print(f\"‚úì Exists: {fname}\")\n",
    "        continue\n",
    "    print(f\"‚Üì Downloading: {fname}\")\n",
    "    urlretrieve(url, dest)\n",
    "\n",
    "# Optional: verify checksums if SHA256SUMS.txt present\n",
    "sumfile = DATA_DIR / \"SHA256SUMS.txt\"\n",
    "if sumfile.exists():\n",
    "    expected = {}\n",
    "    for line in sumfile.read_text().splitlines():\n",
    "        if not line.strip() or line.startswith(\"#\"): \n",
    "            continue\n",
    "        sha, name = line.split(None, 1)\n",
    "        expected[name.strip()] = sha.strip()\n",
    "    for name, sha in expected.items():\n",
    "        fp = DATA_DIR / name\n",
    "        if not fp.exists():\n",
    "            print(f\"Checksum missing file: {name}\")\n",
    "            continue\n",
    "        h = hashlib.sha256(fp.read_bytes()).hexdigest()\n",
    "        print(f\"{name}: {'OK' if h==sha else 'MISMATCH'}\")\n",
    "print(\"Zenodo dataset ready in:\", DATA_DIR)\n"
  ],
  "outputs": [],
  "execution_count": null
},
{
   "cell_type": "markdown",
   "metadata": {
    "id": "V4cLWFDrwKTk"
   },
   "source": [
    "**Welcome to the notebook of \"Translating Akkadian to English using NLP\"!**\n",
    "\n",
    "Please follow the instructions in the following sections in order to get your Akkadian input translated into English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHYA9med86SZ",
    "outputId": "352abd1a-1c40-4be4-f536-3a4b226a7ae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Akkademia'...\n",
      "remote: Enumerating objects: 10658, done.\u001b[K\n",
      "remote: Counting objects: 100% (152/152), done.\u001b[K\n",
      "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
      "remote: Total 10658 (delta 99), reused 140 (delta 91), pack-reused 10506\u001b[K\n",
      "Receiving objects: 100% (10658/10658), 3.28 GiB | 30.45 MiB/s, done.\n",
      "Resolving deltas: 100% (9870/9870), done.\n",
      "Updating files: 100% (7380/7380), done.\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n",
      "Cloning into 'fairseq'...\n",
      "remote: Enumerating objects: 34850, done.\u001b[K\n",
      "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
      "remote: Total 34850 (delta 2), reused 13 (delta 0), pack-reused 34832\u001b[K\n",
      "Receiving objects: 100% (34850/34850), 25.06 MiB | 11.71 MiB/s, done.\n",
      "Resolving deltas: 100% (25297/25297), done.\n",
      "Processing /content/fairseq\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.16.0)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (3.0.4)\n",
      "Collecting hydra-core<1.1,>=1.0.7 (from fairseq==0.12.2)\n",
      "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting omegaconf<2.1 (from fairseq==0.12.2)\n",
      "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.23.5)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2023.6.3)\n",
      "Collecting sacrebleu>=1.4.12 (from fairseq==0.12.2)\n",
      "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.1.0+cu118)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.66.1)\n",
      "Collecting bitarray (from fairseq==0.12.2)\n",
      "  Downloading bitarray-2.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m286.5/286.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.1.0+cu118)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.2.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (23.2)\n",
      "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2)\n",
      "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.5.0)\n",
      "Collecting portalocker (from sacrebleu>=1.4.12->fairseq==0.12.2)\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.9.0)\n",
      "Collecting colorama (from sacrebleu>=1.4.12->fairseq==0.12.2)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.12.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2.1.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->fairseq==0.12.2) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->fairseq==0.12.2) (1.3.0)\n",
      "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
      "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=19951195 sha256=c2fa3898a4f0bacc5e261b44e95d14b48ba7d218294b2badc14aeb35ace88a32\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-9ldlw6wa/wheels/c6/d7/db/bc419b1daa8266aa8de2a7c4d29f62dbfa814e8701fe4695a2\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141209 sha256=b53084f98721693dd91ce439bcaa3a107f0f57e81309eaaf7846413062237600\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
      "Successfully built fairseq antlr4-python3-runtime\n",
      "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
      "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.8.2 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.3.1\n"
     ]
    }
   ],
   "source": [
    "# Please run this section once to prepare your environment to translate Akkadian to English.\n",
    "\n",
    "!git clone https://github.com/gaigutherz/Akkademia.git\n",
    "!cat Akkademia/trans_result.LR_0.1.MAX_TOKENS_4000/checkpoint_best.pt.* > Akkademia/trans_result.LR_0.1.MAX_TOKENS_4000/checkpoint_best.pt\n",
    "!cat Akkademia/not_divided_by_three_dots_result.LR_0.1.MAX_TOKENS_4000/checkpoint_best.pt.* > Akkademia/not_divided_by_three_dots_result.LR_0.1.MAX_TOKENS_4000/checkpoint_best.pt\n",
    "!pip install sentencepiece\n",
    "!git clone https://github.com/pytorch/fairseq\n",
    "!pip install ./fairseq\n",
    "!chmod +x fairseq/fairseq_cli/interactive.py\n",
    "!sed -i 's/#!\\/usr\\/bin\\/env python3 -u/#!\\/usr\\/bin\\/env python3/g' fairseq/fairseq_cli/interactive.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhiSBnsOZ68f",
    "outputId": "fbb4d1e3-b19e-4b97-d2e9-cd818c0a91af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a transliteration sentence for translation\n",
      "Traceback (most recent call last):\n",
      "\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# In this section you can translate one transliteration sentence to English.\n",
    "# Please try to run it first with the sample input, and then you can run it again with input of your own.\n",
    "# Make sure not to use broken\n",
    "# Sample input is: {m}-a≈°-≈°ur‚ÄîBAD‚ÇÉ‚ÄîPAB\n",
    "!python Akkademia/akkadian/translate_transliteration.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8f3sNo8gmo8"
   },
   "outputs": [],
   "source": [
    "# In this section you can translate one cuneiform sentence to English.\n",
    "# Please try to run it first with the sample input, and then you can run it again with input of your own.\n",
    "\n",
    "# Sample input is:  ‚ñÅ . . . íâª íáª íáª\n",
    "!python Akkademia/akkadian/translate_cuneiform.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2UNiHxBe6p8l"
   },
   "outputs": [],
   "source": [
    "# In this section you can translate a transliteration file to English.\n",
    "# Please try to run it first with the sample file, and then you can run it again with files of your own.\n",
    "# The file you use should be in the directory \"Akkademia\".\n",
    "# Please be patient as translation of long files could take a few minutes.\n",
    "\n",
    "# Sample file is: input.tr\n",
    "!python Akkademia/akkadian/translate_from_transliteration.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ykOM6N5j34z"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imNN1PLXiiua"
   },
   "outputs": [],
   "source": [
    "# In this section you can translate a cuneiform file to English.\n",
    "# Please try to run it first with the sample file, and then you can run it again with files of your own.\n",
    "# The file you use should be in the directory \"Akkademia\".\n",
    "# Please be patient as translation of long files could take a few minutes.\n",
    "\n",
    "# Sample file is: input.ak\n",
    "!python Akkademia/akkadian/translate_from_cuneiform.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50xPxx7-r9QX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
